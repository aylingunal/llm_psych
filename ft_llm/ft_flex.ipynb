{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = \"\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which model to ft\n",
    "model_id = \"google/gemma-7b\" # \"meta-llama/Llama-2-7b-hf\"\n",
    "                                      # \"meta-llama/Llama-2-13b-hf\"\n",
    "                                      # \"openai-community/gpt2\"\n",
    "                                      # \"mistralai/Mistral-7B-v0.1\"\n",
    "                                      # \"google/gemma-7b\"\n",
    "                                      # \"tiiuae/falcon-7b\"\n",
    "model_type = \"causal\" # \"sequential\" \"causal\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19be42dcf173414880a238c7fa7e906d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ad8ac0f1f74bf196a8e5a12ca7da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255211af6c8d49aea7d663712e3a8ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b31fe597b234829bf254317322cdd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcac6a840e3464eb2e3e2e52e8df287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8251fca110424dbc9f91c0d9202f332b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8750b4384d3e4e0e925ede15b9dbda2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f91a97202ea4212a83f5f604390fade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9d129746b64995b0f676dd0344c607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4f9a48e6c340468bb108731931bb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e4a889070a482180b47b993a3f6243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209ba8a3dbeb409da67e3bd80a111bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, \\\n",
    "                         AutoModelForCausalLM, \\\n",
    "                         AutoModelForSequenceClassification\n",
    "\n",
    "if model_id == \"meta-llama/Llama-2-7b-hf\" or model_id == \"meta-llama/Llama-2-13b-hf\" :\n",
    "    from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaForSequenceClassification\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = LlamaForCausalLM.from_pretrained(model_id,\n",
    "                                                quantization_config=bnb_config,)\n",
    "    else:\n",
    "        model = LlamaForSequenceClassification.from_pretrained(model_id,\n",
    "                                                quantization_config=bnb_config,)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "elif model_id == \"google/gemma-7b\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "elif model_id == \"mistralai/Mistral-7B-v0.1\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "elif model_id == \"tiiuae/falcon-7b\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "elif model_id == \"openai-community/gpt2\":\n",
    "    from transformers import GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if model_type == \"causal\":\n",
    "        model = GPT2Model.from_pretrained('gpt2')\n",
    "    else:\n",
    "        model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "else:\n",
    "    model = None\n",
    "    tokenizer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lora trainable layers\n",
    "def check_lora_trainable_layers():\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "target_modules = []\n",
    "if (model_id == \"meta-llama/Llama-2-7b-hf\") or \\\n",
    "   (model_id == \"google/gemma-7b\"):\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "elif model_id == \"mistralai/Mistral-7B-v0.1\":\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"down_proj\", \"up_proj\", \"gate_proj\"]\n",
    "elif model_id == \"tiiuae/falcon-7b\":\n",
    "    target_modules = [\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=target_modules,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=['weight']\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e5599723354bcc9910307f598e8338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/12358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# get the data ready\n",
    "from utils import *\n",
    "data_ds = format_dat(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning!\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "output_dirname = \"saved_models/FT_\" + model_id\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "      per_device_train_batch_size=1,\n",
    "      per_device_eval_batch_size=1,\n",
    "      gradient_accumulation_steps=4, # this is for optimization\n",
    "      evaluation_strategy='epoch',\n",
    "      num_train_epochs=1,\n",
    "      warmup_steps=2,\n",
    "     # max_steps=1, # overrides num_train_epochs\n",
    "      learning_rate=2e-4,\n",
    "      fp16=True, # this is for optimization\n",
    "      logging_steps=1,\n",
    "      output_dir=output_dirname,\n",
    "      optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "if model_type == \"causal\":\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_ds['train'],\n",
    "        eval_dataset=data_ds['test'],\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "else:\n",
    "    pass # need to update data_collator for seq cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/gunala/miniconda3/envs/QLORAFTEnv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gunala/miniconda3/envs/QLORAFTEnv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='1235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  69/1235 12:01 < 3:29:08, 0.09 it/s, Epoch 0.06/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8148a9c188d4047a6ff55e86e5b5b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/54.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aegunal/FT_IPD_mistral7b/commit/1f7ccb98187df75fb7f796ca57c01b2a097fc079', commit_message='Upload model', commit_description='', oid='1f7ccb98187df75fb7f796ca57c01b2a097fc079', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = \"\" # write token\n",
    "hub_path = \"aegunal/FT_IPD_gemma7b\" #+ #model_id\n",
    "lora_model.push_to_hub(hub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"saved_models/FT_IPD_mistr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QLORAFTEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
