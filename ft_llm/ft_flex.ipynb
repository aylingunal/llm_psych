{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which model to ft\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # \"meta-llama/Llama-2-7b-hf\"\n",
    "                                      # \"meta-llama/Llama-2-13b-hf\"\n",
    "                                      # \"openai-community/gpt2\"\n",
    "                                      # \"mistralai/Mistral-7B-v0.1\"\n",
    "                                      # \"google/gemma-7b\"\n",
    "                                      # \"tiiuae/falcon-7b\"\n",
    "model_type = \"causal\" # \"sequential\" \"causal\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d430b0b079446b99ce194be38a2b612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, \\\n",
    "                         AutoModelForCausalLM, \\\n",
    "                         AutoModelForSequenceClassification\n",
    "\n",
    "if model_id == \"meta-llama/Llama-2-7b-hf\" or model_id == \"meta-llama/Llama-2-13b-hf\" :\n",
    "    from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaForSequenceClassification\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = LlamaForCausalLM.from_pretrained(model_id,\n",
    "                                                quantization_config=bnb_config,)\n",
    "    else:\n",
    "        model = LlamaForSequenceClassification.from_pretrained(model_id,\n",
    "                                                quantization_config=bnb_config,)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "elif model_id == \"google/gemma-7b\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "elif model_id == \"mistralai/Mistral-7B-v0.1\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "elif model_id == \"tiiuae/falcon-7b\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if model_type == \"causal\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                    quantization_config=bnb_config)\n",
    "elif model_id == \"openai-community/gpt2\":\n",
    "    from transformers import GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    if model_type == \"causal\":\n",
    "        model = GPT2Model.from_pretrained('gpt2')\n",
    "    else:\n",
    "        model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "else:\n",
    "    model = None\n",
    "    tokenizer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=['weight']\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237899fd935a4e08837602e70720e263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/12358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# get the data ready\n",
    "from utils import *\n",
    "data_ds = format_dat(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning!\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "output_dirname = \"saved_models/FT_\" + model_id\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "      per_device_train_batch_size=1,\n",
    "     # gradient_accumulation_steps=4,\n",
    "      evaluation_strategy='epoch',\n",
    "      num_train_epochs=.2, # testing\n",
    "     # warmup_steps=2,\n",
    "      max_steps=1,\n",
    "      learning_rate=2e-4,\n",
    "     # fp16=True,\n",
    "      logging_steps=1,\n",
    "      output_dir=output_dirname,\n",
    "      optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "if model_type == \"causal\":\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_ds['train'],\n",
    "        eval_dataset=data_ds['test'],\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "else:\n",
    "    pass # need to update data_collator for seq cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/gunala/miniconda3/envs/QLORAFTEnv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/gunala/miniconda3/envs/QLORAFTEnv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/155 06:50 < 00:41, 0.34 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = \"\" # write token\n",
    "hub_path = \"aegunal/FT_IPD_\" + model_id\n",
    "lora_model.push_to_hub(\"aegunal/llama13bchat_ft_ipd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QLORAFTEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
