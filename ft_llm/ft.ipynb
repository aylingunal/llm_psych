{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and fine-tune llms on human data\n",
    "\n",
    "here's the colab link --> https://colab.research.google.com/drive/1IFh4vqZRAiiP1aAO2EJBgz7bPVvc2slx#scrollTo=vaiptgAekStt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb6051c39044c7aa032fc858d201304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, \n",
    "                                         quantization_config=bnb_config,\n",
    "                                         #load_in_4bit=True,\n",
    "                                         device_map=\"auto\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lora trainable layers\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "        break\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26214400 || all params: 6698193920 || trainable%: 0.391365199531279\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=['weight']\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# import torch\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760930039d504c429d4904c1926b9d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = \"/home/gunala/LLMDialGen/human_data/human_dat_reformatted.json\"\n",
    "# data processing\n",
    "import json\n",
    "with open(fname,'r') as inf:\n",
    "    human_dat = json.load(inf)\n",
    "\n",
    "def format_history(text,play):\n",
    "  input = f\"\"\"\n",
    "      <<SYS>>\n",
    "      Imagine that you and your criminal partner have been placed under arrest and are being interrogated in separate rooms. Both you and your partner have the option to either 'Silence' or 'Confess'. If both of you are silent, you both receive a prison sentence of 1 year. If one of you confesses and the other stays silent, the partner that confesses will receive a prison sentence of 0 years while the one of the stayed silent will receive a sentence of 5 years. If both of you confess, you will both receive sentences of 3 years. You and your partner will be interrogated for 20 rounds total, and your goal is to minimize overall prison sentence across all rounds. Respond in one word, either 'Silence' or 'Confess'.\\n\n",
    "      <</SYS>>\n",
    "      [INST]\n",
    "      User:{text}\n",
    "      [/INST]\\n\n",
    "\n",
    "      Assistant:{play}\n",
    "  \"\"\"\n",
    "  return input\n",
    "\n",
    "# now will want to reformat each game into llama2-chat friendly format\n",
    "train_texts = {'texts':[]}\n",
    "ind = 0\n",
    "for game_id in human_dat.keys():\n",
    "    round_ids = sorted([int(x) for x in human_dat[game_id].keys()],reverse=True)\n",
    "    game_history = \"\"\n",
    "    for round_id in range(len(round_ids) - 1):\n",
    "        game_history += \"Round \" + str(round_id) + \": \" + \\\n",
    "                        \"Player 1 played \" + human_dat[game_id][str(round_id)]['p1'] + \\\n",
    "                        \", Player 2 played \" + human_dat[game_id][str(round_id)]['p2'] + \". \"\n",
    "    user_prompt = \"You are Player 1, and this is the history of actions so far: \" + game_history + \". Will you choose 'Silence' or 'Confess' in the next round? Please answer in 1 word.\"\n",
    "    play = human_dat[game_id][str(round_id)]['p1']\n",
    "    input = format_history(user_prompt,play)\n",
    "    train_texts['texts'].append(input)\n",
    "\n",
    "    user_prompt = \"You are Player 2, and this is the history of actions so far: \" + game_history + \". Will you choose 'Silence' or 'Confess' in the next round? Please answer in 1 word.\"\n",
    "    play = human_dat[game_id][str(round_id)]['p2']\n",
    "    input = format_history(user_prompt,play)\n",
    "    train_texts['texts'].append(input)\n",
    "\n",
    "    if ind == 5000:\n",
    "        break\n",
    "\n",
    "from datasets import Dataset\n",
    "data = Dataset.from_dict(train_texts)\n",
    "data = data.map(lambda samples: tokenizer(samples['texts']), batched=True)\n",
    "data = data.train_test_split(test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "      per_device_train_batch_size=1,\n",
    "     # gradient_accumulation_steps=4,\n",
    "      evaluation_strategy='epoch',\n",
    "      num_train_epochs=1,\n",
    "     # warmup_steps=2,\n",
    "      max_steps=1,\n",
    "      learning_rate=2e-4,\n",
    "     # fp16=True,\n",
    "      logging_steps=1,\n",
    "      output_dir=\"outputs_train_13bchat\",\n",
    "      optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 21:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.152779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=0.2087707370519638, metrics={'train_runtime': 1304.2957, 'train_samples_per_second': 0.001, 'train_steps_per_second': 0.001, 'total_flos': 29362384281600.0, 'train_loss': 0.2087707370519638, 'epoch': 0.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbde3208bc64184bb8e3b1322bec2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/105M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aegunal/llama13bchat_ft_ipd/commit/af5e08a0e9a161f6e472729947dd1cc0e7cb0183', commit_message='Upload model', commit_description='', oid='af5e08a0e9a161f6e472729947dd1cc0e7cb0183', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = \"\" # need to use write-access token this time\n",
    "lora_model.push_to_hub(\"aegunal/llama13bchat_ft_ipd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inference, can load the model:\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\"aegunal/llama7b_ft_ipd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DialGenEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
