{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and fine-tune llms on human data\n",
    "\n",
    "here's the colab link --> https://colab.research.google.com/drive/1IFh4vqZRAiiP1aAO2EJBgz7bPVvc2slx#scrollTo=vaiptgAekStt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gunala/miniconda3/envs/DialGenEnv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 4096])\n"
     ]
    }
   ],
   "source": [
    "# check lora trainable layers\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "        break\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=['weight']\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"human_dat_reformatted.json\"\n",
    "# data processing\n",
    "import json\n",
    "with open(fname,'r') as inf:\n",
    "    human_dat = json.load(inf)\n",
    "\n",
    "# now will want to reformat each game into NL\n",
    "train_texts = {'texts':[]}\n",
    "for game_id in human_dat.keys():\n",
    "    round_ids = sorted([int(x) for x in human_dat[game_id].keys()],reverse=True)\n",
    "    game_history = \"\"\n",
    "    for round_id in range(len(round_ids)):\n",
    "        game_history += \"Round \" + str(round_id) + \": \" + \\\n",
    "                        \"Player 1 played \" + human_dat[game_id][str(round_id)]['p1'] + \\\n",
    "                        \", Player 2 played \" + human_dat[game_id][str(round_id)]['p2'] + \". \"\n",
    "    train_texts['texts'].append(game_history)\n",
    "\n",
    "from datasets import Dataset\n",
    "data = Dataset.from_dict(train_texts)\n",
    "data = data.map(lambda samples: tokenizer(samples['texts']), batched=True)\n",
    "data = data.train_test_split(test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "      per_device_train_batch_size=1,\n",
    "      gradient_accumulation_steps=4,\n",
    "      evaluation_strategy='epoch',\n",
    "      num_train_epochs=1,\n",
    "      warmup_steps=2,\n",
    "      max_steps=10,\n",
    "      learning_rate=2e-4,\n",
    "      fp16=True,\n",
    "      logging_steps=1,\n",
    "      output_dir=\"outputs\",\n",
    "      optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login() # need to use write-access token this time\n",
    "lora_model.push_to_hub(\"aegunal/llama7b_ft_ipd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inference, can load the model:\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\"aegunal/llama7b_ft_ipd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DialGenEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
